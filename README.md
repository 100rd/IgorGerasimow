Experienced DevOps/Platform engineer with a bias toward automation, simplicity, and trust.
I don’t believe in hero culture or over-engineering. My work is driven by laziness in its best form: building things once, making them reliable, and letting them run with minimal babysitting.

I’ve worked across clouds, clusters, and continents. I enjoy fixing messes others avoid — legacy Terraform states, forgotten Helm releases, half-migrated secrets, and everything in-between.
Think of me as the platform janitor: I come in quietly, clean things up, set up self-healing systems, and leave things better than I found them.

What I value:
 • Automation > repetition
 • Processes that trust people, not gate them
 • Stable systems over shiny ones
 • Honest communication, low ego, high ownership

I’m looking ( might be ) for challenging and bold projects — places where seniority means clarity, not constant firefighting.


Work code exampless are here https://github.com/IgorGerasimow/portfolio



"Perfect pipelines don't exist" — Absolutely agree. DORA's data reinforces this: even "elite" performers have 5% change failure rates. The report explicitly recommends an iterative improvement mindset over chasing some idealized end-state. The best teams focus on continuous improvement, not reaching a "perfect" destination.

"Every tool is a trade-off" — Yes, with emphasis from this year's data. The AI findings are particularly striking here: AI adoption improves flow, productivity, and job satisfaction but decreases software delivery stability by ~7% and throughput by ~1.5%. Platform engineering shows the same pattern — boosts productivity 8% but hurts delivery stability by 14%. Nothing is free.

"Untested = broken" — Strongly agree, and this is implicitly validated throughout the report. They note that AI-generated code is being shipped faster, but without proper test automation, it degrades stability. The vacuum hypothesis they propose suggests teams are shipping more code faster without maintaining testing discipline.

"Communication beats pure skill" — DORA's findings on transformational leadership and documentation quality strongly support this. Documentation quality alone shows 7.5% improvement in product outcomes. Cross-functional coordination, user-centricity, stable priorities — all communication/alignment factors — outperform pure technical capability in predicting success.

"Boring tech wins" — This is partially validated. The cloud flexibility findings are interesting: organizations that moved to cloud without adopting cloud-native practices saw worse outcomes than staying in the data center. The implication: maturity and discipline with your stack matters more than chasing novelty. But I'd nuance this — the report doesn't say "avoid new tech," it says "transform your approach when adopting it."

"DevOps isn't about tools. It's about discipline, empathy, and connecting people with systems that actually work." — This is the central thesis of 10 years of DORA research, honestly. User-centricity is so powerful that high-performing teams don't even need elite delivery speed if they deeply understand user needs. Culture, leadership, stable priorities — all human factors — dominate the predictive models.
What I'd add or refine:

"Small batches are non-negotiable" — The DORA hypothesis for why AI hurts delivery performance is fascinating: AI enables writing more code faster, but teams are shipping larger changelists. This violates one of DORA's most fundamental findings. Speed without discipline backfires.

"Stability of priorities matters more than you think" — Unstable organizational priorities directly increase burnout, and nothing — not good leadership, not documentation, not user-centricity — fully mitigates it. This is a leadership problem, not an engineering one.

"Documentation is not overhead" — Every year DORA finds documentation quality is a massive multiplier. Combined with user-centricity, it amplifies product performance significantly. The Agile manifesto's "working software over comprehensive documentation" gets misread; quality documentation is part of working software.

"Throughput and stability can diverge" — This year's data shows them operating more independently than before. You can be fast but unstable, or slow but reliable. Medium performers actually had better stability than high performers but worse throughput. Which is "better" depends on context and user expectations.
